\chapter{Odometry in the mobile robot}

\begin{figure} [H]

\paragraph*{}

Currently, we have three main sources of information to predict a robot's position: wheel odometry, LiDAR odometry, and an inertial measurement unit (IMU). Each candidate has its own pros and cons in terms of accuracy and efficiency. This section will go through the tests and evaluation of each device and provide solutions to our problems regarding the robot's pose.
\paragraph*{}
We first started investigating the IMU. The IMU we used was the AdaFruit BNO055 which is an IMU that has 9DOF. This is a versatile IMU {Describe the advantage of Adafruit IMU here}. We used an Arduino UNO board to communicate with the IMU. We were able to get the odometry from the IMU by using double Integration on the acceleration to get the x, y, and angular position. However we noticed that the odometry was rifte with innacuracy. This is due mostly because of the integration drift. The inaccuracy means we cannot solely depend on the IMU for odometry.

\paragraph*{}
The next odometry we investigated is the LIDAR odometry. For this we used a ros2 package called rf20-Laser. Every time the Lidar scans a point, a range flow constraint equation is formulated for each scanned point in terms of sensor velocity, and a robust function of the resulting geometric constraints is minimized to estimate motion. Unlike traditional methods, this approach does not rely on correspondences but instead performs dense scan alignment using scan gradients, similar to dense 3D visual odometry. After installing the ros2 package and running it, we found out that the odometry measurement is very accurate. However, the odometry is prone to drifting when the lidar is scanning a moving object which in our case will likely happen since we have three robots. 

\paragraph*{}
For wheel encoder odometry in four-wheeled omnidirectional mobile robots, the motor controls each wheel independently, resulting in total control of the robot's movements in the 2-D plane. The measurements from the rotational movements of the encoders can help us determine the distance traveled by the wheel through mathematical calculations.
AX-12W presents a complete interface for reading and writing the motor status. As for wheel odometry, the main data we are interested in specifically are the present position and speed of the motor. With a baud rate of 1M, and the ability to bulk-read all motors instantaneously, we presume that our motors will be well adapted to the robot's pose estimation model. \cite{phunopas2018motion}

\paragraph*{}
Sensor fusion is essential due to the inherent limitations of individual sensors. Wheel odometry is susceptible to wheel slippage and drift over time, LiDAR odometry performs well but is prone to errors when scanning dynamic objects, and IMU data suffers from integration drift. By fusing these sensors, their strengths can complement each other, mitigating individual weaknesses and improving overall accuracy. The Extended Kalman Filter (EKF) is the optimal solution for sensor fusion in this context because it effectively handles non-linear motion models while incorporating uncertainty from different sources. EKF recursively estimates the robotâ€™s state by predicting its position using a motion model and continuously updating it with sensor measurements, ensuring a more stable and accurate pose estimation. We could the feed this odometry to to SLAM to achieve the best possible mapping.

\end{figure}
