\chapter{Object Detection}

\paragraph*{}
The object detection system initially developed in the previous semester successfully detected a yellow cylinder, generated a bounding box around it, aligned the box’s center with the camera frame, and measured the distance between the robot and the object using the camera’s focal length. However, this implementation relied on the assumption that the cylinder’s dimensions were known, and LiDAR had not yet been integrated. Additionally, the scope of object detection was refined from hexagonal prisms and cubes to cylinders to eliminate the need for pose estimation and edge detection, which would otherwise introduce additional computational complexity for dynamic object grasping.

\paragraph*{}
In transitioning to real-world implementation, several modifications were made to the object detection workflow. The system now identifies three distinct objects: a yellow, a blue, and a red cylinder, each differing in size. At this stage, only a single object is placed in the arena at a time, with detection based primarily on colour. Once detected, the system generates a bounding box around the object, and its distance and size are measured using an S3 RPLiDAR. To enhance accuracy, multi-sensor fusion between the camera and LiDAR is employed. By leveraging the camera’s horizontal field of view (FOV), the system maps the bounding box’s pixel range to the corresponding angle and retrieves the appropriate LiDAR data based on the robot’s odometry.

\paragraph*{}
The current implementation has successfully demonstrated the ability to detect objects based on color while filtering out reflections from the floor. Initial testing was conducted under controlled lighting conditions, including illumination from the left, right, front, and back. The presence of vibrations caused by the camera mount during movement did not significantly affect detection performance. However, at this stage, accuracy assessments are still based on visual inspection, and further validation will be conducted once the model undergoes proper calibration with the sensors. Overall, progress remains on schedule and aligns with the expected timeline, with each milestone being achieved as planned.

\paragraph*{}
In measuring distance and size, image distortion remains a key factor influencing accuracy, as the system must correctly map the detected object’s bounding box angle to real-world coordinates. Since the object’s height does not contribute to measurement calculations, vertical calibration data from the camera has been omitted. Additionally, the top 30 degrees of the camera frame is cropped to remove potential noise and prevent false detections of unintended objects or individuals outside the arena.

\paragraph*{}
One of the primary challenges encountered in this phase is object occlusion, where multiple objects appearing in the frame can lead to overlapping bounding boxes. In such cases, the robot is required to reposition itself to separate the detected objects and ensure accurate size measurements without interference. Another challenge is ensuring reliable color detection under varying lighting conditions, as different light sources and angles can affect how the camera perceives object colors. Additionally, image distortion presents difficulties in mapping the bounding box to the correct position in relation to the LiDAR data. These obstacles are being addressed through careful calibration, adjustments in object positioning, and improvements to the detection algorithm to enhance its robustness against occlusion and lighting variations.

\paragraph*{}
To meet the minimum viable product (MVP) requirements, the initial implementation is limited to detecting a single object within the arena. The system must be capable of generating a bounding box, measuring both distance and angle, and determining the object's size using LiDAR. Once these fundamental objectives are achieved, the next phase of testing will involve detecting multiple objects within a single frame while handling occlusion through coordinated robot movement.

\paragraph*{}
Further testing will focus on optimizing the model and evaluating its accuracy under different conditions. The model will first be validated under the MVP scenario, dealing with only one object, before advancing to a multi-object setting with occlusion management. To assess distance measurement accuracy, objects will be placed at 0.5, 1, and 3 meters from the robot, allowing evaluation of the alignment between LiDAR and camera data. Additionally, eight different lighting conditions will be introduced by positioning the light source at varying angles to examine the robustness of the color detection model under diverse illumination environments.
