\chapter{Localization}
\section*{SLAM Analysis for Robot Navigation}

\paragraph*{}
Simultaneous Localization and Mapping (SLAM) is a computational technique used by mobile robots to build a map of an unknown environment while simultaneously estimating their own position within that map. SLAM fuses data from sensors such as LiDAR, IMU, and wheel encoders to incrementally construct a geometric representation of the surrounding environment. By performing scan matching and correcting odometry drift over time, SLAM enables reliable localization and autonomous navigation even in GPS-denied environments. Variants like Graph SLAM use nonlinear optimization to refine pose graphs, while real-time solutions such as SLAM Toolbox offer online mapping and loop closure capabilities.

In our project, SLAM was intended to support three cooperative robots performing collective object transport. We explored both Graph SLAM and SLAM Toolbox as potential solutions. SLAM Toolbox was particularly appealing due to its integration with ROS 2 and support for asynchronous global pose graph optimization. The goal was to construct a shared map, correct odometry drift from both LiDAR (RF2O) and encoder sources, and enable reliable path planning across the arena. SLAM would also assist with multi-robot coordination by aligning their reference frames to a common global map, which is crucial for tasks such as role assignment and distributed transport.

However, SLAM cannot be effectively deployed in our current setup due to a combination of environmental and sensor limitations. The arena's physical dimensions (approximately $2.1 \, \text{m} \times 1.4 \, \text{m}$) are too small to allow sufficient motion for loop closures or robust scan alignment. Moreover, the environment lacks distinctive features, providing little geometric structure for scan matching algorithms to latch onto. On the odometry side, wheel encoders on the Dynamixel-based drive system have blind spots, and the RF2O LiDAR odometry exhibits significant drift and instability in low-feature settings. These issues with the odometry is further described below.

\newpage
\section*{Wheel Odometry}
\paragraph*{}
\quad Wheel odometry is solely calculated using the dynamic model of the omnidirectional wheeled robot. Hence, the position or the velocity of the wheel becomes a necessity. However, wheel odometry is only suitable under two conditions: the robot must not have wheel slip, and the encoder must be precise. 
\paragraph*{}
The problem that arose during the implementation phase was that the motor (Dynamixel AX-12W) was not able to return the complete position and velocity values. Due to the limited range of the embedded encoder in the motor, the resulting random value is between 330º and 350º (Figure \ref{fig:goal-position}). Moreover, the value returned by the encoder, according to the documentation, was the percentage of the maximum torque. \cite{robotisAX12W} 

\begin{figure}[H]
    \centering
    \includegraphics[width=0.35\linewidth]{assets/images/odometry/goal_position.png}
    \caption{Encoder diagram of AX-12W}
    \label{fig:goal-position}
\end{figure}

To work around this, we instead tried the current speed register. However, this approach requires the robot to operate at relatively low speeds to maintain accuracy. Yet, this method will not work during the collective transport phase, as the load to the motor will be different. Hence, LiDAR odometry was considered as the more viable solution for odometry.

\newpage
\section*{LiDAR Odometry}
The LiDAR odometery package that we used was the MAPIRlab rf20 odometer package. The details of the package were described in their paper published in the ICRA conference. The process used for the calculation of LiDAR odometry was based on range flow on velocity estimates. 

\subsection*{1. Range Flow Constraint}

Assuming a static environment, the total derivative of the range measurement $r(\theta, t)$ at angle $\theta$ and time $t$ is zero:

\[
\frac{d r(\theta, t)}{dt} = \frac{\partial r}{\partial \theta} \frac{d\theta}{dt} + \frac{\partial r}{\partial t} = 0
\]

\subsection*{2. Velocity Twist of the Sensor}

We represent the robot’s planar motion as a twist vector:

\[
\mathbf{v} = 
\begin{bmatrix}
v_x \\
v_y \\
\omega
\end{bmatrix}
\]

Each point $(x_i, y_i)$ from the laser scan moves according to:

\[
\dot{x}_i = v_x - \omega y_i, \quad \dot{y}_i = v_y + \omega x_i
\]

\subsection*{3. Range Derivative from Motion}

Let $r_i = \sqrt{x_i^2 + y_i^2}$ be the range. The predicted temporal change in range is:

\[
\dot{r}_i = \frac{x_i \dot{x}_i + y_i \dot{y}_i}{r_i}
\]

Substituting $\dot{x}_i$ and $\dot{y}_i$:

\[
\dot{r}_i = \frac{x_i (v_x - \omega y_i) + y_i (v_y + \omega x_i)}{r_i}
\]

\[
\dot{r}_i = \frac{v_x x_i + v_y y_i + \omega (x_i y_i - y_i x_i)}{r_i} = \frac{v_x x_i + v_y y_i}{r_i}
\]

(The cross-term cancels out.)

\subsection*{4. Least-Squares Estimation}

Given measured temporal derivatives $\dot{r}_i^{\text{meas}}$, the goal is to find $\mathbf{v}$ that minimizes:

\[
E(\mathbf{v}) = \sum_i \left( \dot{r}_i^{\text{meas}} - \dot{r}_i^{\text{pred}} \right)^2
\]

This results in a linear system of the form:

\[
A \mathbf{v} = b
\]

which is solved using robust linear least-squares methods.

\subsection*{Problems and issues}
In our experiments with RF2O-based LiDAR odometry, we encountered several critical issues. Firstly, the robot's orientation appeared inconsistent over time, with the estimated $x$ and $y$ positions occasionally swapping, leading to significant localization errors. Secondly, the system continued to update the pose incrementally even when the robot was completely stationary, indicating motion when none occurred. Lastly, during actual movement, the estimated pose exhibited significant drift from the ground truth trajectory, compromising the accuracy of the robot's localization.

The root causes of these problems can be traced to inherent limitations of RF2O. The orientation inconsistencies are due to the absence of an absolute reference frame—RF2O cannot distinguish between rotated coordinate frames without external input, leading to ambiguity when the robot turns. The false motion during stationarity arises from sensor noise and small variations in consecutive scans, which RF2O interprets as real movement. The drift during motion is a result of accumulating small scan alignment errors, especially in low-feature environments, due to the purely incremental nature of RF2O's pose estimation. To further asses the issue we decided to validate LiDAR odometery using an overhead camera.
\section*{Odometry Validation}
\quad The performance evaluation of the LiDAR odometry is carried out using an overhead camera setup. To ensure that our odometry is accurate enough for use in SLAM, the camera tracks the robot’s movement and provides a ground-truth odometry reference. Then, the data from the camera are compared to the LiDAR's.

The testing was carried out in a controlled indoor environment using a Logitech C922 PRO HD Stream webcam mounted on a studio stand (Figure \ref{fig:camera-setting}). The tests were carried out on a flat surface free from slipperiness and reflections(Figure \ref{fig:arena-setting}). 
\begin{figure}[!htb]
    \centering
    \begin{minipage}{0.48\textwidth}
        \centering
        \includegraphics[height=3.5cm]{assets/images/odometry/cam_setting.jpg}
        \caption{Camera setup}
        \label{fig:camera-setting}
    \end{minipage}\hfill
    \begin{minipage}{0.48\textwidth}
        \centering
        \includegraphics[height=3.5cm]{assets/images/odometry/arena_setting.jpg}
        \caption{Arena setup}
        \label{fig:arena-setting}
    \end{minipage}
\end{figure}

The Augmented Reality University of Cordoba (ArUco) markers are used to locate the robot's position within the testing arena. DICT\_4×4\_50 ArUco markers were selected for this testing because they're the least complex markers, making computation faster and more reliable for real-time detection. 

The ArUco markers were placed directly above the robot hardware, while the camera faced directly into the arena (Figure \ref{fig:aruco-swarm}). After that, our overhead camera system extracted the ArUcos from each camera frame and calculated the actual position from the map using the Perspective-n-Point (PnP) pose computation.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.35\linewidth]{assets/images/odometry/aruco.jpg}
    \caption{ArUco markers on the swarms}
    \label{fig:aruco-swarm}
\end{figure}

\subsection*{ArUco Marker Localization}

Given the four detected corner points of an ArUco marker in image coordinates:
\[
\text{Corners} = \{ \mathbf{p}_1, \mathbf{p}_2, \mathbf{p}_3, \mathbf{p}_4 \} = \{ \text{topLeft}, \text{topRight}, \text{bottomRight}, \text{bottomLeft} \}
\]

\paragraph*{1. Center of the Marker}
\[
c_x = \frac{x_{\text{topLeft}} + x_{\text{bottomRight}}}{2}, \quad
c_y = \frac{y_{\text{topLeft}} + y_{\text{bottomRight}}}{2}
\]

\paragraph*{2. Direction Vector of the Top Edge}
\[
\Delta x = x_{\text{topRight}} - x_{\text{topLeft}}, \quad
\Delta y = y_{\text{topRight}} - y_{\text{topLeft}}
\]

\paragraph*{3. Orientation Estimation}
Display orientation in degrees:
\[
\theta_{\text{display}} = \arctan2(\Delta y, -\Delta x)
\]

\paragraph*{4. Normalized Image Coordinates}
Assuming image width $W$ and height $H$:
\[
x_{\text{norm}} = \frac{c_x}{W}, \quad
y_{\text{norm}} = \frac{c_y}{H}
\]

\paragraph*{5. Real-World Position Mapping}
Given real-world map dimensions $S_x$ and $S_y$:
\[
x_{\text{map}} = (x_{\text{norm}} - 0.5) \cdot S_x, \quad
y_{\text{map}} = -(y_{\text{norm}} - 0.5) \cdot S_y
\]

The given formulation allows the transformation of detected 2D marker positions into real-world coordinates as figure shown:

\begin{figure}[H]
    \centering
    \includegraphics[width=1\linewidth]{assets/images/odometry/mapping.png}
    \caption{Visualization of real-time mapping of swarms using OpenCV and Turtle}
    \label{fig:coor-mapping}
\end{figure}

\newpage
After obtaining data from both the camera and LiDAR, we evaluated the trajectory error produced by the LiDAR odometry through the following tests:

\begin{enumerate}
    \item \textbf{Translational motion} (holonomic): The robot moved 50 cm in the positive \(x\)-direction via sideways motion (45° to the traveling direction).
    \item \textbf{Translational motion} (non-holonomic): The robot moved 50 cm in the positive \(x\)-direction via forward motion (0° to the traveling direction).
    \item \textbf{Rotational motion}: The robot performed a 360° turn clockwise and counterclockwise in place.
\end{enumerate}


\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\linewidth]{assets/images/odometry/testing_visual.png}
    \caption{Visualization of robot motion based on LiDAR and camera odometry in a 2D plane.}
    \label{fig:visual-result}
\end{figure}

As shown in Figure\ref{fig:visual-result}, the robot’s ability to move in a straight line significantly deteriorated when switching from moving at a 45° angle to the direction of travel (using two wheels) to moving in the direction it is facing (using all four wheels). The goal of this analysis was to evaluate whether the LiDAR odometry could accurately capture trajectory errors and be reliably used for SLAM or feedback control.

However, one issue encountered was that LiDAR odometry assigned a different x-axis orientation for each test. To address this, we compensated for the discrepancy by remapping the x-axis to match the robot’s initial direction of motion during error calculation.

Figure\ref{fig:detail-result} presents the detailed numerical comparison of the robot's estimated position and orientation ($x$, $y$, and $\theta$) from both the LiDAR and camera odometry across all test scenarios.


\begin{figure}[H]
    \centering
    \includegraphics[width=1\linewidth]{assets/images/odometry/detail_visual.png}
    \caption{Detailed Visualization of LiDAR vs actual odometry}
    \label{fig:detail-result}
\end{figure}

\begin{table}[H]
\centering
\begin{tabular}{|c|c|c|c|}
\hline
\textbf{Metric} & \textbf{Translational (45°)} & \textbf{Translational (0°)} & \textbf{Rotational} \\
\hline
NRMSE (Y) & 0.483 & 0.597 & 1.282 \\
NRMSE (X) & 0.389 & 0.614 & 0.768 \\
NRMSE ($\theta$) & 0.360 & 0.629 & 0.407 \\
\hline
\end{tabular}
\caption{Normalised Root Mean Squared Error (NRMSE) Comparison Between Camera and LiDAR}
\label{tab:mse-results}
\end{table}

From our experiments, we observed that the current LiDAR odometry often deviates from the actual trajectory, especially in movements that involve rotation or sliding. These inconsistencies show that the LiDAR setup may not provide sufficient accuracy for precise localization tasks or feedback data. As a result, we propose using the overhead camera system with ArUco markers as a temporary but reliable substitute for ground-truth positioning. This approach offers improved consistency and will serve as our main reference until a more robust odometry solution is implemented.
