\chapter{Graph SLAM}

\begin{enumerate}
    \item \textbf{Create the initial graph using odometry:}
    \[
    x_{t} = x_{i} + \Delta s \cos(\theta_{i}),
    \]
    \[
    y_{t} = y_{i} + \Delta s \sin(\theta_{i}),
    \]
    \[
    \theta_{t} = \theta_{i} + \Delta \theta
    \]
    where $\Delta s$ is the distance traveled. 
    
    The edge $e_{ij}$ is represented as:
    \[
    e_{ij} = 
    \begin{bmatrix}
        \Delta x \\
        \Delta y \\
        \Delta \theta
    \end{bmatrix}.
    \]

    \item \textbf{Loop closure}
    
    When a robot recognises a previous landmark, an additional edge is added between non-consecutive poses $x_{i}$ and $x_{j}$ where $j \neq i + 1$. This is a sensor based measurement, not odometry.
    \[
    e_{ij} = \begin{bmatrix}
        \Delta x_{ij}\\
        \Delta y_{ij}\\
        \Delta \theta_{ij}
    \end{bmatrix}
    \]

    \item \textbf{Defining the error function/residual:}
    \[
    e_{ij} = 
    \begin{bmatrix}
        x_{j} - (x_{i} + \Delta x_{ij}) \\
        y_{j} - (y_{i} + \Delta y_{ij}) \\
        \theta_{j} - (\theta_{i} + \Delta \theta_{ij})
    \end{bmatrix}.
    \]
    Here, $x_{j}, y_{j}, \theta_{j}$ are the current noisy estimates, and $x_{i} + \Delta x_{ij}, y_{i} + \Delta y_{ij}, \theta_{i} + \Delta \theta_{ij}$ are the expected values for pose $j$ based on pose $i$ (from $[i]$) and the expected transformation $\Delta x_{ij}, \Delta y_{ij}, \Delta \theta_{ij}$.

    \item \textbf{Objective function/Sum of squared errors:}
    \[
    f(x) = \sum_{(i,j) \in \text{edges}} e_{ij}(x_{i}, x_{j})^{T} \Omega_{ij} e_{ij}(x_{i}, x_{j}),
    \]
    where $\Omega_{ij}$ is the inverse of the covariance matrix, called the information matrix. It measures how confident we are in our measurements. By multiplying with the residuals, we can scale the error based on our confidence.

    - If the measurement is noisy, $\Omega_{ij}$ will have smaller values.
    - Otherwise, $\Omega_{ij}$ will have high values.

    Example of $\Omega_{ij}$:
    \[
    \Omega_{ij} = 
    \begin{bmatrix}
        \omega_x & 0 & 0 \\
        0 & \omega_y & 0 \\
        0 & 0 & \omega_\theta
    \end{bmatrix},
    \]
    where $\omega_x$, $\omega_y$, and $\omega_\theta$ are the weighted confidences in $x$, $y$, and $\theta$, respectively.

    The information matrix is used because SLAM uses batch optimization compared to the covariance matrix, which is mainly used in sequential updates.

    \item \textbf{Gauss-Newton:}
    \begin{enumerate}
        \item Linearize using Taylor expansion:
        \[
        e_{ij}(x_{i}, x_{j}) \approx e_{ij}(x_{i0}, x_{j0}) + J_{ij}(x_{i} - x_{i0}),
        \]
        where $J_{ij} = \frac{\partial e_{ij}}{\partial x}\bigg|_{x=x_{0}}$.

        \item Substitute the linearized term into the objective function:
        \[
        f(x) = \sum_{(i,j) \in \text{edges}} \left[ e_{ij}(x_{i0}, x_{j0}) + J_{ij}(x_{i} - x_{i0}) \right]^T \Omega_{ij} \left[ e_{ij}(x_{i0}, x_{j0}) + J_{ij}(x_{i} - x_{i0}) \right].
        \]

        \item Expand the terms:
        \[
        f(x) \approx \sum_{(i,j) \in \text{edges}} \left[ e_{ij}(x_{i0}, x_{j0})^T \Omega_{ij} e_{ij}(x_{i0}, x_{j0}) + 2 e_{ij}(x_{i0}, x_{j0})^T \Omega_{ij} J_{ij} (x - x_0) + (x - x_0)^T J_{ij}^T \Omega_{ij} J_{ij} (x - x_0) \right].
        \]
        Recognize that $f(x)$ has now become a quadratic function
    \end{enumerate}

    \item \textbf{Gauss-Newton Normal Equations:}
    To minimize $f(x)$, solve the normal equations:
    \[
    H \Delta x = -g,
    \]
    where 
    \[
    H = \sum_{(i,j) \in \text{edges}} J_{ij}^T \Omega_{ij} J_{ij} \quad \text{(Hessian approximation)},
    \]
    \[
    g = \sum_{(i,j) \in \text{edges}} J_{ij}^T \Omega_{ij} e_{ij}(x_{i0}, x_{j0}) \quad \text{(gradient vector)}.
    \]

    The Hessian approximation is the sum of the outer products of Jacobians weighted by $\Omega_{ij}$. The gradient vector is the sum of the Jacobian-weighted residuals.

    \textbf{Steps to solve:}
    \begin{enumerate}
        \item Decompose $H$ using Cholesky decomposition:
        \[
        H = L L^T,
        \]
        where $L$ is the lower triangular matrix.
        \item Solve for $y$ using:
        \[
        L y = -g.
        \]
        \item Solve for $\Delta x$ using:
        \[
        L^T \Delta x = y.
        \]
    \end{enumerate}

    After obtaining $\Delta x$, update:
    \[
    x \leftarrow x + \Delta x.
    \]

    Check for convergence by ensuring $f(x)$ is below a predefined threshold. If so the algorithm has converged and it can stop.
    Else, once again compute the residuals and the Jacobian.
    \subsection*{Future SLAM Implementation}
    As described above, we have already outlined a clear plan to approach SLAM. Our approach begins with implementing the solution in a Webots simulation using a third party github repository. Landmarks will be identified and labeled with either numbers or names, serving as reference points for navigation. Initially, we will use cameras for object detection as a placeholder, with plans to incorporate a LIDAR sensor in the future.
\end{enumerate}

