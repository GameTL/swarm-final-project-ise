\chapter{Object Detection using Computer Vision}

\paragraph*{}
The object detection module consists of three main components: color-based detection using the HSV color space, sensor fusion, and the measurement of distance, angle, and object diameter. All three modules have been completed and are ready for integration into the robot for testing in the collaborative transport task.

\paragraph*{}
Based on the progress outlined in the previous report, the experimental arena will consist of a single cylinder to mitigate challenges associated with object occlusion and the complexity of multiple object detection. The camera and 2D RPLiDAR have been calibrated by aligning the position of the camera directly under the LiDAR, ensuring their alignment along both the horizontal and vertical axes. The angle is determined by calculating the difference between the center of the camera and the center of the bounding box, focusing solely on the x-coordinate in the horizontal axis. This calculated angle is then mapped into the LiDAR coordinate space to derive the actual angle for the LiDAR, enabling it to retrieve the real-world distance between the center of the LiDAR and the center of the detected cylinder. The angle mapping between the camera and LiDAR is achieved using the Field of View (FOV) from the camera calibration performed using OpenCV, as expressed in the following formula:

\[
\text{relative\_angle} = \left( \frac{\text{object\_center\_x} - \text{frame\_center\_x}}{\text{frame\_width}} \right) \times \text{horizontal\_fov}
\]

\paragraph*{}
Additionally, The workflow involves stopping the robot once the center of a bounding box along the horizontal axis falls within a tolerance zone of ±7\% (equivalent to 600 pixels from both the left and right edges of the frame). Due to image distortion at the frame borders, the width of the bounding box increases significantly compared to the width measured at the center.

\paragraph*{}
Once the center of the bounding box is within the tolerance zone and the detected object's color falls within the predefined range, the object detection status is set to \textit{Object Found}. At this point, the robot halts and begins measuring the distance from the LiDAR center to the surface of the cylinder, which is aligned with the horizontal center and perpendicular to the LiDAR sensor. The distance and angle between the sensors and the object are determined using the camera's field of view (FOV) to transform the angle from the camera space to the LiDAR space, ensuring accurate distance measurements in meters.

\paragraph*{}
After determining the distance, the next step is to estimate the object's diameter using the bounding box width and distance. Despite restricting vision to the 7\% tolerance zone, some distortion still affects the bounding box width. The actual width of the object is computed using the following equation:

\begin{equation}
\text{Actual Width} = \tan\left(\frac{\text{Angle Difference}}{2}\right) \times \text{Distance}
\end{equation}

\paragraph*{}
The actual width varies by ±2 cm as the distance changes due to image distortion at different proximities to the camera. To mitigate this error, a variation factor function is introduced, multiplying it with the actual width calculation to maintain consistency across different distances. The updated equation is:

\begin{equation}
\text{Actual Width} = \tan\left(\frac{\text{Angle Difference}}{2}\right) \times \text{Distance} \times k
\end{equation}

\paragraph*{}
The variation factor is computed for each distance-angle pair, followed by quadratic regression with distance as the independent variable and the variation factor as the dependent variable. The obtained regression formula is:

\begin{equation}
y = 0.0344768745x^2 - 0.1910787305x + 1.2288527888
\end{equation}

\paragraph*{}
After obtaining the variation function, its accuracy is evaluated. The calculated metrics include a Mean Squared Error (MAE) of 0.005 and an R-squared value of 0.7957. Additionally, the MSE for the final model is approximately 0.00024.

\paragraph*{}
Testing was conducted through basic visual inspection by placing the object at various distances from the sensors. The observed error was ±0.7 cm, indicating a need for further model optimization. Potential sources of error include lighting conditions and insufficient data at certain distances.

\paragraph*{}
For optimization, the current model is trained using data from cylinders with diameters of 20 cm and 23 cm. A dataset for a 16 cm diameter cylinder has been collected and preprocessed, but further regression analysis is required. Additionally, the training dataset should be balanced across different distances, ensuring an equal number of samples per distance. Another optimization step involves reducing the tolerance zone to enhance accuracy.

\paragraph*{}
The next step is to conduct comprehensive testing under varying lighting conditions and distances (0.5 m, 1 m, 2 m, and 3 m). The experiments will be performed under eight different lighting conditions for all three cylinder sizes (16 cm, 20 cm, and 23 cm) and are scheduled to be completed within the first week of April.
